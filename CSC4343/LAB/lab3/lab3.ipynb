{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":659,"status":"ok","timestamp":1712266593464,"user":{"displayName":"Jian Zhang","userId":"12524632998994188518"},"user_tz":300},"id":"wkoV4iraq0b5","outputId":"fe86299a-24e5-4b41-ae99-c13187897a6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["%pylab is deprecated, use %matplotlib inline and import the required libraries.\n","Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["%pylab inline"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"gPYLC2qVMwMm"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"5kdQO3g_q0b9"},"source":["### Task 1\n","\n","Write a function **word_2_vec(L)**. It takes a list L of strings (you can assume that the strings contain only lower-case letters, no number or special characters), and translate each string into a matrx (2d array) of nx26 where n is the number of letters in the string. (Different strings may have different length and thus different n.) Each row of the matrix is a one-hot vector (length 26) encoding the corresponding letter. For example, the encoding vector for the letter 'a' should be a vector of all zeros expect that the first entry should be one. The encoding vector for 'b' should have the second entry be one. The function should return a list of matrices, each corresponds to a string in L.   \n","\n","[Use Python and Numpy only. No other packages. If you don't want to build by hand a letter-to-number dictionary, You may use Python function **ord(l)** to obtain the ascii value of the letter l."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"fNEPk3sZq0cA"},"outputs":[],"source":["def char_2_one_hot(c):\n","    index = ord(c) - ord('a')\n","    return [1 if i == index else 0 for i in range(26)]\n","\n","def word_2_vec(L):\n","    return [[char_2_one_hot(c) for c in word] for word in L]"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"UV70TRMdq0cA"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"]}],"source":["for m in word_2_vec(['abc', 'to', 'vec']):\n","    print(m)"]},{"cell_type":"markdown","metadata":{"id":"az9U6pdFq0cB"},"source":["### IMDb dataset\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8466,"status":"ok","timestamp":1712266051632,"user":{"displayName":"Jian Zhang","userId":"12524632998994188518"},"user_tz":300},"id":"p98BPNmaAjz4","outputId":"62d43712-731d-423a-bd15-d2c4d0637f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["25000 train sequences\n","25000 test sequences\n"]}],"source":["from keras.datasets import imdb\n","\n","max_features = 20000  # use only this number of words (most common words) and ignore others\n","maxlen = 80  # make all review the same length (cutting longer ones and padding shorter ones)\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1712273733096,"user":{"displayName":"Jian Zhang","userId":"12524632998994188518"},"user_tz":300},"id":"e9Aje3MkCkaH","outputId":"5d39aa31-fe32-47df-af3d-a6c5bff4209a"},"outputs":[{"name":"stdout","output_type":"stream","text":["1 19999\n"]}],"source":["min_id = min([min(x) for x in x_train])\n","max_id = max([max(x) for x in x_train])\n","print(min_id, max_id)"]},{"cell_type":"markdown","metadata":{"id":"4FNPLZCzq0cC"},"source":["### Task 2\n","\n","Build a model to classify text. The inputs are imdb reviews and your model should read a review and predict whether the review is positive (1) or negative (0). Note that in the data, each review is a list of numbers (not words).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IeeXmoE5Dafi"},"source":["#### Task 2.1\n","\n","Preprocess the data to make the sequence the same length (as defined by maxlen above).\n","After preprocessing, turn x_train and x_test into tensors so that you can create PyTorch dataloaders from these tensors."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"YGCrbyz2q0cE"},"outputs":[],"source":["\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader, TensorDataset\n","\n","x_train_padded = pad_sequence(\n","    [torch.tensor(seq) for seq in x_train], batch_first=True, padding_value=0).numpy()\n","x_test_padded = pad_sequence(\n","    [torch.tensor(seq) for seq in x_test], batch_first=True, padding_value=0).numpy()\n","\n","x_train_padded = torch.tensor(x_train_padded[:, :maxlen])\n","x_test_padded = torch.tensor(x_test_padded[:, :maxlen])\n","\n","y_train_tensor = torch.tensor(y_train)\n","y_test_tensor = torch.tensor(y_test)\n","\n","train_data = TensorDataset(x_train_padded, y_train_tensor)\n","test_data = TensorDataset(x_test_padded, y_test_tensor)\n","\n","train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"P-wFFlWQq0cE"},"source":["#### Task 2.2\n","\n","Build a model (subclass of nn.Module) with two layers of LSTM (each has 128 neurons/cells) for the text classification. Note that:\n"," - We don't use the word number as input directly. You need to use an embedding layer (nn.Embedding) to translate the word (number) into an vector. (You can set the length of the vector to be 128.)  \n"," - After going through the LSTM layers, give the LSTM output at the last position to a linear output layer to compute the probability of positive/negative.\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"DAYnQ0unTEoT"},"outputs":[],"source":["class MovieReviewClassifier(nn.Module):\n","    def __init__(self):\n","        super(MovieReviewClassifier, self).__init__()\n","        self.embedding = nn.Embedding(\n","            num_embeddings=max_features, embedding_dim=128)\n","        self.lstm = nn.LSTM(input_size=128, hidden_size=128,\n","                            num_layers=2, batch_first=True)\n","        self.out = nn.Linear(128, 1)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        _, (hidden, _) = self.lstm(x)\n","        x = hidden[-1]\n","        return torch.sigmoid(self.out(x))\n","\n","\n","model = MovieReviewClassifier()"]},{"cell_type":"markdown","metadata":{"id":"uLKvGNuFBhg3"},"source":["#### Task 2.3\n","\n","After building your model, train it (20 epoches) on the train data and test it on the test data. Print out the accuracy on the test data.  "]},{"cell_type":"code","execution_count":17,"metadata":{"id":"2hgeGrHBBrhr"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1, Accuracy: 0.56968\n","Epoch 2, Accuracy: 0.7348\n","Epoch 3, Accuracy: 0.7688\n","Epoch 4, Accuracy: 0.79588\n","Epoch 5, Accuracy: 0.7892\n","Epoch 6, Accuracy: 0.7898\n","Epoch 7, Accuracy: 0.782\n","Epoch 8, Accuracy: 0.776\n","Epoch 9, Accuracy: 0.77452\n","Epoch 10, Accuracy: 0.7782\n","Epoch 11, Accuracy: 0.77616\n","Epoch 12, Accuracy: 0.76404\n","Epoch 13, Accuracy: 0.77208\n","Epoch 14, Accuracy: 0.7772\n","Epoch 15, Accuracy: 0.77228\n","Epoch 16, Accuracy: 0.76996\n","Epoch 17, Accuracy: 0.77196\n","Epoch 18, Accuracy: 0.76928\n","Epoch 19, Accuracy: 0.7692\n","Epoch 20, Accuracy: 0.76976\n"]}],"source":["from torch.optim import Adam\n","from sklearn.metrics import accuracy_score\n","\n","criterion = nn.BCELoss()\n","optimizer = Adam(model.parameters(), lr=0.001)\n","\n","for epoch in range(20):\n","    model.train()\n","    for inputs, labels in train_loader:\n","        optimizer.zero_grad()\n","        outputs = model(inputs).squeeze()\n","        loss = criterion(outputs, labels.float())\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    all_predictions = []\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            outputs = model(inputs).squeeze()\n","            predictions = (outputs >= 0.5).long()\n","            all_predictions.extend(predictions.numpy())\n","    accuracy = accuracy_score(y_test, all_predictions)\n","    print(f'Epoch {epoch+1}, Accuracy: {accuracy}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
